{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "077cb76a-72eb-41cd-bf4f-5d058b27ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"sara-nabhani/google-flan-t5-small-e-snli-generation-label_and_explanation-selected-b48\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"sara-nabhani/google-flan-t5-small-e-snli-generation-label_and_explanation-selected-b48\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152fc522-3f8c-4987-b277-e043ee9eb42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/home/ec2-user/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ff2758cf2a4d1396943b4103e46135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the e-SNLI dataset\n",
    "dataset = load_dataset(\"esnli\")\n",
    "\n",
    "#train_dataset = dataset['train']\n",
    "eval_dataset = dataset['validation']\n",
    "#test_dataset = dataset['test']\n",
    "\n",
    "#indices = list(range(0, len(train_dataset), 10))  # Select every 10th index\n",
    "#train_dataset = train_dataset.select(indices)\n",
    "\n",
    "#len(train_dataset), len(eval_dataset)#, len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a60d2c-a0f9-4991-8554-f39f83009d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dct = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
    "dct = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2399b27e-6031-4627-9979-0063b39c1915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841ca82c73b2473ab3951f9d14c82fc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9842 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# Remove batch dimension\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m],  \u001b[38;5;66;03m# Remove batch dimension\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_encoding[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# Remove batch dimension\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     }\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#train_dataset = train_dataset.map(\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#    preprocess,\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#    remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m \u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpremise\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhypothesis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexplanation_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexplanation_2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexplanation_3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2346\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   2343\u001b[0m disable_tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_proc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2361\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2366\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2368\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:532\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:499\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    497\u001b[0m }\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 499\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/fingerprint.py:458\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m             kwargs[fingerprint_name] \u001b[38;5;241m=\u001b[39m update_fingerprint(\n\u001b[1;32m    453\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fingerprint, transform, kwargs_for_fingerprint\n\u001b[1;32m    454\u001b[0m             )\n\u001b[1;32m    456\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 458\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2715\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[1;32m   2713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[1;32m   2714\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pbar):\n\u001b[0;32m-> 2715\u001b[0m         example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2716\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   2717\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2614\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   2613\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 2614\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;66;03m# Check if the function returns updated examples\u001b[39;00m\n\u001b[1;32m   2617\u001b[0m     update_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, (Mapping, pa\u001b[38;5;241m.\u001b[39mTable))\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/datasets/arrow_dataset.py:2306\u001b[0m, in \u001b[0;36mDataset.map.<locals>.decorate.<locals>.decorated\u001b[0;34m(item, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2302\u001b[0m decorated_item \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2303\u001b[0m     Example(item, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched \u001b[38;5;28;01melse\u001b[39;00m Batch(item, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m   2304\u001b[0m )\n\u001b[1;32m   2305\u001b[0m \u001b[38;5;66;03m# Use the LazyDict internally, while mapping the function\u001b[39;00m\n\u001b[0;32m-> 2306\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecorated_item\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;66;03m# Return a standard dict\u001b[39;00m\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, LazyDict) \u001b[38;5;28;01melse\u001b[39;00m result\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m      9\u001b[0m output_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_dct[example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation_1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation_2\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation_3\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Tokenize input and output\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m input_encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m output_encoding \u001b[38;5;241m=\u001b[39m tokenizer(output_text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Create a dictionary to return\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2802\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2800\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2801\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2802\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2908\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2889\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2890\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2905\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2906\u001b[0m     )\n\u001b[1;32m   2907\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2911\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2916\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2981\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2972\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2973\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   2974\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2979\u001b[0m )\n\u001b[0;32m-> 2981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3000\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils.py:722\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    720\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_for_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfirst_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpair_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msecond_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3471\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.prepare_for_model\u001b[0;34m(self, ids, pair_ids, add_special_tokens, padding, truncation, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, prepend_batch_axis, **kwargs)\u001b[0m\n\u001b[1;32m   3468\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_length:\n\u001b[1;32m   3469\u001b[0m     encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(encoded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 3471\u001b[0m batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoded_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\n\u001b[1;32m   3473\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:223\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    219\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 223\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:748\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    745\u001b[0m     value \u001b[38;5;241m=\u001b[39m [value]\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 748\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    754\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m tensor\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:720\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    # Prepare input and output text\n",
    "    input_text = f\"Premise: {example['premise']} Hypothesis: {example['hypothesis']} What is the relationship? Explain your answer.\"\n",
    "    output_text = f\"{label_dct[example['label']]}: {example['explanation_1']}. {example['explanation_2']}. {example['explanation_3']}.\"\n",
    "\n",
    "    # Tokenize input and output\n",
    "    input_encoding = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    output_encoding = tokenizer(output_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Create a dictionary to return\n",
    "    return {\n",
    "        \"input_ids\": input_encoding[\"input_ids\"][0],  # Remove batch dimension\n",
    "        \"attention_mask\": input_encoding[\"attention_mask\"][0],  # Remove batch dimension\n",
    "        \"labels\": output_encoding[\"input_ids\"][0] # Remove batch dimension\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "#train_dataset = train_dataset.map(\n",
    "#    preprocess,\n",
    "#    remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\n",
    "#)\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372d9b9a-7a57-425c-abd9-0f8aaba5e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parsing\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def dependency_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    # Generate the dependency tree structure\n",
    "    return \" \".join([f\"<{token.dep_}> {token.text}\" for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c7e0d4-744d-4e96-9aaa-4feb5c6a7133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f30a1537107449bb99788a1a1421818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9842 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    # Prepare input and output text\n",
    "    input_text = f\"Premise: example['premise'] Dependency: {dependency_parse(example['premise'])} Hypothesis: example['hypothesis'] Dependency: {dependency_parse(example['hypothesis'])} What is the relationship? Explain your answer.\"\n",
    "    output_text = f\"{label_dct[example['label']]}: {example['explanation_1']}. {example['explanation_2']}. {example['explanation_3']}.\"\n",
    "\n",
    "    # Tokenize input and output\n",
    "    input_encoding = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    output_encoding = tokenizer(output_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Create a dictionary to return\n",
    "    return {\n",
    "        \"input_ids\": input_encoding[\"input_ids\"][0],  # Remove batch dimension\n",
    "        \"attention_mask\": input_encoding[\"attention_mask\"][0],  # Remove batch dimension\n",
    "        \"labels\": output_encoding[\"input_ids\"][0] # Remove batch dimension\n",
    "    }\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc67906a-3140-49e1-9f02-d79c85e50c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ea7904-bc5d-40e0-9da6-5da2552c7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from datasets import load_metric\n",
    "\n",
    "# Define the metrics you want to compute\n",
    "rouge = load_metric(\"rouge\")\n",
    "bleu = load_metric(\"bleu\")\n",
    "bertscore = load_metric(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc03f7b8-4544-473d-9443-a1ebe52935dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1717 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3434 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5151 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6868 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8585 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10302 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12019 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13736 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15453 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17170 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18887 : 9842 9842 9842 9842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20604 : 9842 9842 9842 9842\n"
     ]
    }
   ],
   "source": [
    "full_dct = {}\n",
    "\n",
    "for checkpoint_num in [1717, 3434, 5151, 6868, 8585, 10302, 12019, 13736, 15453, 17170, 18887, 20604]:\n",
    "\n",
    "    # Specify the checkpoint path (local or Hugging Face Hub)\n",
    "    checkpoint_path = \"../expt4/flan_t5_esnli/checkpoint-\" + str(checkpoint_num)\n",
    "    \n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(checkpoint_path).cuda()\n",
    "    \n",
    "    dec_outputs = []\n",
    "    labels = []\n",
    "    for i in range(9842):\n",
    "        en_inputs = eval_dataset[i]\n",
    "        \n",
    "        # Get the model's output\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(en_inputs['input_ids'].cuda().unsqueeze(0), attention_mask=en_inputs['attention_mask'].cuda().unsqueeze(0))\n",
    "        # Decode the output (convert token IDs back to text)\n",
    "        decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        dec_outputs.append(decoded_output)\n",
    "        # Get original labels\n",
    "        orig = dataset['validation'][i]\n",
    "        labels.append(f\"{label_dct[orig['label']]}: {orig['explanation_1']}. {orig['explanation_2']}. {orig['explanation_3']}.\")\n",
    "    \n",
    "    exact_preds = []\n",
    "    exact_labels = []\n",
    "    for i in range(9842):\n",
    "        exact_preds.append(dec_outputs[i].split(':')[0])\n",
    "        exact_labels.append(labels[i].split(':')[0])\n",
    "    print(checkpoint_num, \":\", len(dec_outputs), len(labels), len(exact_preds), len(exact_labels))\n",
    "    \n",
    "    accuracy = accuracy_score(exact_labels, exact_preds)\n",
    "    precision = precision_score(exact_labels, exact_preds, average='weighted')\n",
    "    recall = recall_score(exact_labels, exact_preds, average='weighted')\n",
    "    f1 = f1_score(exact_labels, exact_preds, average='weighted')\n",
    "    rouge_results = rouge.compute(predictions=dec_outputs, references=labels)\n",
    "    bleu_results = bleu.compute(predictions=[tokenizer.tokenize(dout) for dout in dec_outputs],\n",
    "                                references=[[tokenizer.tokenize(lbl)] for lbl in labels])\n",
    "    bertscore_results = bertscore.compute(predictions=dec_outputs, references=labels, lang='en')\n",
    "    \n",
    "    tmpdct = {}\n",
    "    tmpdct['Accuracy'] = accuracy\n",
    "    tmpdct['Precision'] = precision\n",
    "    tmpdct['Recall'] = recall\n",
    "    tmpdct['F1'] = f1\n",
    "    tmpdct['rouge1'] = rouge_results['rouge1'].mid.fmeasure\n",
    "    tmpdct['rouge2'] = rouge_results['rouge2'].mid.fmeasure\n",
    "    tmpdct['rougeL'] = rouge_results['rougeL'].mid.fmeasure\n",
    "    tmpdct['rougeLsum'] = rouge_results['rougeLsum'].mid.fmeasure\n",
    "    tmpdct['bleu'] = bleu_results['bleu']\n",
    "    tmpdct['bertscore'] = np.mean(bertscore_results['f1'])\n",
    "\n",
    "    full_dct[checkpoint_num] = tmpdct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d346d344-a704-40d9-9610-bdc25894a6cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>bleu</th>\n",
       "      <th>bertscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.785003</td>\n",
       "      <td>0.791357</td>\n",
       "      <td>0.785003</td>\n",
       "      <td>0.785505</td>\n",
       "      <td>0.305976</td>\n",
       "      <td>0.164003</td>\n",
       "      <td>0.274187</td>\n",
       "      <td>0.274276</td>\n",
       "      <td>0.022325</td>\n",
       "      <td>0.888484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>0.817618</td>\n",
       "      <td>0.824828</td>\n",
       "      <td>0.817618</td>\n",
       "      <td>0.819133</td>\n",
       "      <td>0.319454</td>\n",
       "      <td>0.177425</td>\n",
       "      <td>0.286806</td>\n",
       "      <td>0.286852</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.890284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>0.832554</td>\n",
       "      <td>0.833016</td>\n",
       "      <td>0.832554</td>\n",
       "      <td>0.832282</td>\n",
       "      <td>0.330475</td>\n",
       "      <td>0.185782</td>\n",
       "      <td>0.295853</td>\n",
       "      <td>0.295892</td>\n",
       "      <td>0.028017</td>\n",
       "      <td>0.891558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>0.844239</td>\n",
       "      <td>0.844850</td>\n",
       "      <td>0.844239</td>\n",
       "      <td>0.844395</td>\n",
       "      <td>0.342030</td>\n",
       "      <td>0.196331</td>\n",
       "      <td>0.304345</td>\n",
       "      <td>0.304402</td>\n",
       "      <td>0.032545</td>\n",
       "      <td>0.891581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>0.843528</td>\n",
       "      <td>0.842901</td>\n",
       "      <td>0.843528</td>\n",
       "      <td>0.842887</td>\n",
       "      <td>0.330851</td>\n",
       "      <td>0.185366</td>\n",
       "      <td>0.295123</td>\n",
       "      <td>0.295233</td>\n",
       "      <td>0.028791</td>\n",
       "      <td>0.891713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10302</th>\n",
       "      <td>0.845763</td>\n",
       "      <td>0.847990</td>\n",
       "      <td>0.845763</td>\n",
       "      <td>0.846361</td>\n",
       "      <td>0.332409</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.296334</td>\n",
       "      <td>0.296301</td>\n",
       "      <td>0.027983</td>\n",
       "      <td>0.892246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>0.852266</td>\n",
       "      <td>0.854154</td>\n",
       "      <td>0.852266</td>\n",
       "      <td>0.852762</td>\n",
       "      <td>0.330483</td>\n",
       "      <td>0.184768</td>\n",
       "      <td>0.295101</td>\n",
       "      <td>0.295127</td>\n",
       "      <td>0.028651</td>\n",
       "      <td>0.891449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13736</th>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.851642</td>\n",
       "      <td>0.851351</td>\n",
       "      <td>0.851427</td>\n",
       "      <td>0.330287</td>\n",
       "      <td>0.184699</td>\n",
       "      <td>0.295236</td>\n",
       "      <td>0.295243</td>\n",
       "      <td>0.028157</td>\n",
       "      <td>0.891786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15453</th>\n",
       "      <td>0.852266</td>\n",
       "      <td>0.851935</td>\n",
       "      <td>0.852266</td>\n",
       "      <td>0.851993</td>\n",
       "      <td>0.330611</td>\n",
       "      <td>0.185170</td>\n",
       "      <td>0.295392</td>\n",
       "      <td>0.295439</td>\n",
       "      <td>0.027812</td>\n",
       "      <td>0.892709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17170</th>\n",
       "      <td>0.854196</td>\n",
       "      <td>0.855304</td>\n",
       "      <td>0.854196</td>\n",
       "      <td>0.854583</td>\n",
       "      <td>0.333317</td>\n",
       "      <td>0.185642</td>\n",
       "      <td>0.297187</td>\n",
       "      <td>0.297251</td>\n",
       "      <td>0.029121</td>\n",
       "      <td>0.892344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18887</th>\n",
       "      <td>0.854298</td>\n",
       "      <td>0.855029</td>\n",
       "      <td>0.854298</td>\n",
       "      <td>0.854585</td>\n",
       "      <td>0.330141</td>\n",
       "      <td>0.182960</td>\n",
       "      <td>0.294323</td>\n",
       "      <td>0.294431</td>\n",
       "      <td>0.028334</td>\n",
       "      <td>0.891878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20604</th>\n",
       "      <td>0.853282</td>\n",
       "      <td>0.853439</td>\n",
       "      <td>0.853282</td>\n",
       "      <td>0.853346</td>\n",
       "      <td>0.329741</td>\n",
       "      <td>0.182290</td>\n",
       "      <td>0.294199</td>\n",
       "      <td>0.294283</td>\n",
       "      <td>0.028024</td>\n",
       "      <td>0.891996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall        F1    rouge1    rouge2    rougeL  \\\n",
       "1717   0.785003   0.791357  0.785003  0.785505  0.305976  0.164003  0.274187   \n",
       "3434   0.817618   0.824828  0.817618  0.819133  0.319454  0.177425  0.286806   \n",
       "5151   0.832554   0.833016  0.832554  0.832282  0.330475  0.185782  0.295853   \n",
       "6868   0.844239   0.844850  0.844239  0.844395  0.342030  0.196331  0.304345   \n",
       "8585   0.843528   0.842901  0.843528  0.842887  0.330851  0.185366  0.295123   \n",
       "10302  0.845763   0.847990  0.845763  0.846361  0.332409  0.186000  0.296334   \n",
       "12019  0.852266   0.854154  0.852266  0.852762  0.330483  0.184768  0.295101   \n",
       "13736  0.851351   0.851642  0.851351  0.851427  0.330287  0.184699  0.295236   \n",
       "15453  0.852266   0.851935  0.852266  0.851993  0.330611  0.185170  0.295392   \n",
       "17170  0.854196   0.855304  0.854196  0.854583  0.333317  0.185642  0.297187   \n",
       "18887  0.854298   0.855029  0.854298  0.854585  0.330141  0.182960  0.294323   \n",
       "20604  0.853282   0.853439  0.853282  0.853346  0.329741  0.182290  0.294199   \n",
       "\n",
       "       rougeLsum      bleu  bertscore  \n",
       "1717    0.274276  0.022325   0.888484  \n",
       "3434    0.286852  0.025000   0.890284  \n",
       "5151    0.295892  0.028017   0.891558  \n",
       "6868    0.304402  0.032545   0.891581  \n",
       "8585    0.295233  0.028791   0.891713  \n",
       "10302   0.296301  0.027983   0.892246  \n",
       "12019   0.295127  0.028651   0.891449  \n",
       "13736   0.295243  0.028157   0.891786  \n",
       "15453   0.295439  0.027812   0.892709  \n",
       "17170   0.297251  0.029121   0.892344  \n",
       "18887   0.294431  0.028334   0.891878  \n",
       "20604   0.294283  0.028024   0.891996  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#df = pd.DataFrame.from_dict(full_dct, orient='index')\n",
    "#df.to_csv('finetuned_metrics_dep_parse_2.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a02887e0-e242-40fb-a8bd-bc835fb19a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>bleu</th>\n",
       "      <th>bertscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.767019</td>\n",
       "      <td>0.780916</td>\n",
       "      <td>0.767019</td>\n",
       "      <td>0.767135</td>\n",
       "      <td>0.313550</td>\n",
       "      <td>0.170333</td>\n",
       "      <td>0.279890</td>\n",
       "      <td>0.279807</td>\n",
       "      <td>0.024491</td>\n",
       "      <td>0.887953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>0.821378</td>\n",
       "      <td>0.822594</td>\n",
       "      <td>0.821378</td>\n",
       "      <td>0.821819</td>\n",
       "      <td>0.319514</td>\n",
       "      <td>0.177734</td>\n",
       "      <td>0.286904</td>\n",
       "      <td>0.286757</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.890620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>0.827779</td>\n",
       "      <td>0.828226</td>\n",
       "      <td>0.827779</td>\n",
       "      <td>0.826749</td>\n",
       "      <td>0.326923</td>\n",
       "      <td>0.182265</td>\n",
       "      <td>0.292942</td>\n",
       "      <td>0.292872</td>\n",
       "      <td>0.027338</td>\n",
       "      <td>0.891194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>0.840276</td>\n",
       "      <td>0.839744</td>\n",
       "      <td>0.840276</td>\n",
       "      <td>0.839798</td>\n",
       "      <td>0.339354</td>\n",
       "      <td>0.192991</td>\n",
       "      <td>0.301622</td>\n",
       "      <td>0.301540</td>\n",
       "      <td>0.031543</td>\n",
       "      <td>0.891103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>0.843325</td>\n",
       "      <td>0.842824</td>\n",
       "      <td>0.843325</td>\n",
       "      <td>0.842701</td>\n",
       "      <td>0.334203</td>\n",
       "      <td>0.188652</td>\n",
       "      <td>0.297687</td>\n",
       "      <td>0.297604</td>\n",
       "      <td>0.030350</td>\n",
       "      <td>0.891516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10302</th>\n",
       "      <td>0.850640</td>\n",
       "      <td>0.852886</td>\n",
       "      <td>0.850640</td>\n",
       "      <td>0.851195</td>\n",
       "      <td>0.331297</td>\n",
       "      <td>0.185622</td>\n",
       "      <td>0.296071</td>\n",
       "      <td>0.295976</td>\n",
       "      <td>0.027745</td>\n",
       "      <td>0.892516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>0.849218</td>\n",
       "      <td>0.848959</td>\n",
       "      <td>0.849218</td>\n",
       "      <td>0.848645</td>\n",
       "      <td>0.329392</td>\n",
       "      <td>0.183935</td>\n",
       "      <td>0.294692</td>\n",
       "      <td>0.294547</td>\n",
       "      <td>0.028410</td>\n",
       "      <td>0.891766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13736</th>\n",
       "      <td>0.850030</td>\n",
       "      <td>0.850412</td>\n",
       "      <td>0.850030</td>\n",
       "      <td>0.850042</td>\n",
       "      <td>0.328779</td>\n",
       "      <td>0.182998</td>\n",
       "      <td>0.293859</td>\n",
       "      <td>0.293751</td>\n",
       "      <td>0.027111</td>\n",
       "      <td>0.892260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15453</th>\n",
       "      <td>0.852875</td>\n",
       "      <td>0.852487</td>\n",
       "      <td>0.852875</td>\n",
       "      <td>0.852558</td>\n",
       "      <td>0.330694</td>\n",
       "      <td>0.184610</td>\n",
       "      <td>0.295569</td>\n",
       "      <td>0.295427</td>\n",
       "      <td>0.028042</td>\n",
       "      <td>0.892244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17170</th>\n",
       "      <td>0.852875</td>\n",
       "      <td>0.853543</td>\n",
       "      <td>0.852875</td>\n",
       "      <td>0.853109</td>\n",
       "      <td>0.333664</td>\n",
       "      <td>0.185779</td>\n",
       "      <td>0.297787</td>\n",
       "      <td>0.297660</td>\n",
       "      <td>0.028886</td>\n",
       "      <td>0.892454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18887</th>\n",
       "      <td>0.855416</td>\n",
       "      <td>0.855871</td>\n",
       "      <td>0.855416</td>\n",
       "      <td>0.855598</td>\n",
       "      <td>0.331268</td>\n",
       "      <td>0.184005</td>\n",
       "      <td>0.295638</td>\n",
       "      <td>0.295521</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.892045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20604</th>\n",
       "      <td>0.855619</td>\n",
       "      <td>0.855599</td>\n",
       "      <td>0.855619</td>\n",
       "      <td>0.855609</td>\n",
       "      <td>0.330903</td>\n",
       "      <td>0.183432</td>\n",
       "      <td>0.295109</td>\n",
       "      <td>0.295023</td>\n",
       "      <td>0.028164</td>\n",
       "      <td>0.892041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall        F1    rouge1    rouge2    rougeL  \\\n",
       "1717   0.767019   0.780916  0.767019  0.767135  0.313550  0.170333  0.279890   \n",
       "3434   0.821378   0.822594  0.821378  0.821819  0.319514  0.177734  0.286904   \n",
       "5151   0.827779   0.828226  0.827779  0.826749  0.326923  0.182265  0.292942   \n",
       "6868   0.840276   0.839744  0.840276  0.839798  0.339354  0.192991  0.301622   \n",
       "8585   0.843325   0.842824  0.843325  0.842701  0.334203  0.188652  0.297687   \n",
       "10302  0.850640   0.852886  0.850640  0.851195  0.331297  0.185622  0.296071   \n",
       "12019  0.849218   0.848959  0.849218  0.848645  0.329392  0.183935  0.294692   \n",
       "13736  0.850030   0.850412  0.850030  0.850042  0.328779  0.182998  0.293859   \n",
       "15453  0.852875   0.852487  0.852875  0.852558  0.330694  0.184610  0.295569   \n",
       "17170  0.852875   0.853543  0.852875  0.853109  0.333664  0.185779  0.297787   \n",
       "18887  0.855416   0.855871  0.855416  0.855598  0.331268  0.184005  0.295638   \n",
       "20604  0.855619   0.855599  0.855619  0.855609  0.330903  0.183432  0.295109   \n",
       "\n",
       "       rougeLsum      bleu  bertscore  \n",
       "1717    0.279807  0.024491   0.887953  \n",
       "3434    0.286757  0.025208   0.890620  \n",
       "5151    0.292872  0.027338   0.891194  \n",
       "6868    0.301540  0.031543   0.891103  \n",
       "8585    0.297604  0.030350   0.891516  \n",
       "10302   0.295976  0.027745   0.892516  \n",
       "12019   0.294547  0.028410   0.891766  \n",
       "13736   0.293751  0.027111   0.892260  \n",
       "15453   0.295427  0.028042   0.892244  \n",
       "17170   0.297660  0.028886   0.892454  \n",
       "18887   0.295521  0.028333   0.892045  \n",
       "20604   0.295023  0.028164   0.892041  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#df = pd.DataFrame.from_dict(full_dct, orient='index')\n",
    "#df.to_csv('finetuned_metrics_dep_parse.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d348efca-4ddf-4059-b3d0-986c30dd0710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "      <th>rougeLsum</th>\n",
       "      <th>bleu</th>\n",
       "      <th>bertscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1717</th>\n",
       "      <td>0.791506</td>\n",
       "      <td>0.798316</td>\n",
       "      <td>0.791506</td>\n",
       "      <td>0.792216</td>\n",
       "      <td>0.315346</td>\n",
       "      <td>0.172856</td>\n",
       "      <td>0.282249</td>\n",
       "      <td>0.282218</td>\n",
       "      <td>0.025040</td>\n",
       "      <td>0.888709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3434</th>\n",
       "      <td>0.823816</td>\n",
       "      <td>0.824766</td>\n",
       "      <td>0.823816</td>\n",
       "      <td>0.824090</td>\n",
       "      <td>0.319376</td>\n",
       "      <td>0.177217</td>\n",
       "      <td>0.286602</td>\n",
       "      <td>0.286584</td>\n",
       "      <td>0.024744</td>\n",
       "      <td>0.890823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5151</th>\n",
       "      <td>0.837127</td>\n",
       "      <td>0.837127</td>\n",
       "      <td>0.837127</td>\n",
       "      <td>0.837037</td>\n",
       "      <td>0.328025</td>\n",
       "      <td>0.183608</td>\n",
       "      <td>0.294088</td>\n",
       "      <td>0.294112</td>\n",
       "      <td>0.027381</td>\n",
       "      <td>0.891864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>0.845560</td>\n",
       "      <td>0.845990</td>\n",
       "      <td>0.845560</td>\n",
       "      <td>0.845673</td>\n",
       "      <td>0.340730</td>\n",
       "      <td>0.194530</td>\n",
       "      <td>0.303295</td>\n",
       "      <td>0.303381</td>\n",
       "      <td>0.032375</td>\n",
       "      <td>0.891209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8585</th>\n",
       "      <td>0.847287</td>\n",
       "      <td>0.847026</td>\n",
       "      <td>0.847287</td>\n",
       "      <td>0.847099</td>\n",
       "      <td>0.329061</td>\n",
       "      <td>0.184114</td>\n",
       "      <td>0.294050</td>\n",
       "      <td>0.294056</td>\n",
       "      <td>0.028093</td>\n",
       "      <td>0.891675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10302</th>\n",
       "      <td>0.848202</td>\n",
       "      <td>0.852512</td>\n",
       "      <td>0.848202</td>\n",
       "      <td>0.849256</td>\n",
       "      <td>0.326484</td>\n",
       "      <td>0.180799</td>\n",
       "      <td>0.292041</td>\n",
       "      <td>0.292144</td>\n",
       "      <td>0.025782</td>\n",
       "      <td>0.892846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12019</th>\n",
       "      <td>0.856025</td>\n",
       "      <td>0.858295</td>\n",
       "      <td>0.856025</td>\n",
       "      <td>0.856407</td>\n",
       "      <td>0.328617</td>\n",
       "      <td>0.183529</td>\n",
       "      <td>0.294337</td>\n",
       "      <td>0.294406</td>\n",
       "      <td>0.027895</td>\n",
       "      <td>0.892001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13736</th>\n",
       "      <td>0.851555</td>\n",
       "      <td>0.852861</td>\n",
       "      <td>0.851555</td>\n",
       "      <td>0.851977</td>\n",
       "      <td>0.328562</td>\n",
       "      <td>0.181739</td>\n",
       "      <td>0.293510</td>\n",
       "      <td>0.293569</td>\n",
       "      <td>0.026576</td>\n",
       "      <td>0.892368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15453</th>\n",
       "      <td>0.857041</td>\n",
       "      <td>0.856827</td>\n",
       "      <td>0.857041</td>\n",
       "      <td>0.856883</td>\n",
       "      <td>0.328780</td>\n",
       "      <td>0.182764</td>\n",
       "      <td>0.294289</td>\n",
       "      <td>0.294323</td>\n",
       "      <td>0.026860</td>\n",
       "      <td>0.892717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17170</th>\n",
       "      <td>0.858667</td>\n",
       "      <td>0.859386</td>\n",
       "      <td>0.858667</td>\n",
       "      <td>0.858937</td>\n",
       "      <td>0.333516</td>\n",
       "      <td>0.185581</td>\n",
       "      <td>0.297573</td>\n",
       "      <td>0.297627</td>\n",
       "      <td>0.028458</td>\n",
       "      <td>0.892863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18887</th>\n",
       "      <td>0.860191</td>\n",
       "      <td>0.860824</td>\n",
       "      <td>0.860191</td>\n",
       "      <td>0.860416</td>\n",
       "      <td>0.331042</td>\n",
       "      <td>0.183445</td>\n",
       "      <td>0.295487</td>\n",
       "      <td>0.295493</td>\n",
       "      <td>0.028149</td>\n",
       "      <td>0.892330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20604</th>\n",
       "      <td>0.858667</td>\n",
       "      <td>0.858833</td>\n",
       "      <td>0.858667</td>\n",
       "      <td>0.858740</td>\n",
       "      <td>0.331162</td>\n",
       "      <td>0.183476</td>\n",
       "      <td>0.295567</td>\n",
       "      <td>0.295598</td>\n",
       "      <td>0.028154</td>\n",
       "      <td>0.892407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Accuracy  Precision    Recall        F1    rouge1    rouge2    rougeL  \\\n",
       "1717   0.791506   0.798316  0.791506  0.792216  0.315346  0.172856  0.282249   \n",
       "3434   0.823816   0.824766  0.823816  0.824090  0.319376  0.177217  0.286602   \n",
       "5151   0.837127   0.837127  0.837127  0.837037  0.328025  0.183608  0.294088   \n",
       "6868   0.845560   0.845990  0.845560  0.845673  0.340730  0.194530  0.303295   \n",
       "8585   0.847287   0.847026  0.847287  0.847099  0.329061  0.184114  0.294050   \n",
       "10302  0.848202   0.852512  0.848202  0.849256  0.326484  0.180799  0.292041   \n",
       "12019  0.856025   0.858295  0.856025  0.856407  0.328617  0.183529  0.294337   \n",
       "13736  0.851555   0.852861  0.851555  0.851977  0.328562  0.181739  0.293510   \n",
       "15453  0.857041   0.856827  0.857041  0.856883  0.328780  0.182764  0.294289   \n",
       "17170  0.858667   0.859386  0.858667  0.858937  0.333516  0.185581  0.297573   \n",
       "18887  0.860191   0.860824  0.860191  0.860416  0.331042  0.183445  0.295487   \n",
       "20604  0.858667   0.858833  0.858667  0.858740  0.331162  0.183476  0.295567   \n",
       "\n",
       "       rougeLsum      bleu  bertscore  \n",
       "1717    0.282218  0.025040   0.888709  \n",
       "3434    0.286584  0.024744   0.890823  \n",
       "5151    0.294112  0.027381   0.891864  \n",
       "6868    0.303381  0.032375   0.891209  \n",
       "8585    0.294056  0.028093   0.891675  \n",
       "10302   0.292144  0.025782   0.892846  \n",
       "12019   0.294406  0.027895   0.892001  \n",
       "13736   0.293569  0.026576   0.892368  \n",
       "15453   0.294323  0.026860   0.892717  \n",
       "17170   0.297627  0.028458   0.892863  \n",
       "18887   0.295493  0.028149   0.892330  \n",
       "20604   0.295598  0.028154   0.892407  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "#df = pd.DataFrame.from_dict(full_dct, orient='index')\n",
    "#df.to_csv('finetuned_metrics.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b640a77f-c6a1-471a-8843-1facbe511ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('finetuned_metrics.csv')\n",
    "df2 = pd.read_csv('finetuned_metrics_dep_parse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3840586-9241-4831-b2f2-00f87ca526bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f057c5ba650>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Create the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the columns from both DataFrames\n",
    "plt.plot(list(range(1,13)), df1['bertscore'], label='orig', color='blue', marker='o')  # Plot df1\n",
    "plt.plot(list(range(1,13)), df2['bertscore'], label='depparse', color='red', marker='x')   # Plot df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49764234-e638-4ea4-b322-159593680453",
   "metadata": {},
   "outputs": [],
   "source": [
    "Most optimal fine-tuned model selected based on loss:\n",
    "Accuracy: 0.8378\n",
    "Precision (Macro): 0.8382\n",
    "Recall (Macro): 0.8375\n",
    "F1-score (Macro): 0.8374\n",
    "\n",
    "My own fine-tuned model epoch 6:\n",
    "Accuracy: 0.8551\n",
    "Precision (Macro): 0.8548\n",
    "Recall (Macro): 0.8548\n",
    "F1-score (Macro): 0.8547\n",
    "\n",
    "Directly loaded pretrained:\n",
    "Accuracy: 0.7040\n",
    "Precision (Macro): 0.7708\n",
    "Recall (Macro): 0.7057\n",
    "F1-score (Macro): 0.7060"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
