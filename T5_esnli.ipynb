{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c435b4eb-e013-41a9-b249-c6b529d0bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed) \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209a9384-e298-4d45-bf94-1358f5e0fda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/home/ec2-user/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6a7d893167b487cafbc18faf87ed033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(54937, 9842)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the e-SNLI dataset\n",
    "dataset = load_dataset(\"esnli\")\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['validation']\n",
    "#test_dataset = dataset['test']\n",
    "\n",
    "indices = list(range(0, len(train_dataset), 10))  # Select every 10th index\n",
    "train_dataset = train_dataset.select(indices)\n",
    "\n",
    "len(train_dataset), len(eval_dataset)#, len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b32aa8a6-22bc-4244-8e4d-c02afaa87a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dct = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c7af12c-8b0d-4506-badf-299f43de1aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3b84a50b194331bd889cea5173a561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54937 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b86d61a278d44dea46e6721d3a5d425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9842 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    # Prepare input and output text\n",
    "    input_text = f\"Premise: {example['premise']} Hypothesis: {example['hypothesis']} What is the relationship? Explain your answer.\"\n",
    "    output_text = f\"{label_dct[example['label']]}: {example['explanation_1']}. {example['explanation_2']}. {example['explanation_3']}.\"\n",
    "\n",
    "    # Tokenize input and output\n",
    "    input_encoding = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    output_encoding = tokenizer(output_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Create a dictionary to return\n",
    "    return {\n",
    "        \"input_ids\": input_encoding[\"input_ids\"][0],  # Remove batch dimension\n",
    "        \"attention_mask\": input_encoding[\"attention_mask\"][0],  # Remove batch dimension\n",
    "        \"labels\": output_encoding[\"input_ids\"][0] # Remove batch dimension\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67dd7885-7e99-470f-9c32-53972b6e1ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print(train_dataset[0]['input_ids'].shape)  # Should show (512,)\n",
    "print(train_dataset[0]['attention_mask'].shape)  # Should show (512,)\n",
    "print(train_dataset[0]['labels'].shape)  # Should show (512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bfbeca9-e34b-42b6-bb9c-03cfb0855da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a866afc1-a48c-4ccc-b544-423512348148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "# Define custom optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan_t5_esnli\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of every epoch\n",
    "    save_strategy=\"epoch\",  # Save at the end of every epoch\n",
    "    per_device_train_batch_size=8,  # Train batch size\n",
    "    per_device_eval_batch_size=8,  # Evaluation batch size\n",
    "    num_train_epochs=12,  # Number of epochs\n",
    "    learning_rate=learning_rate,  # Learning rate\n",
    "    lr_scheduler_type=\"linear\",  # Linear learning rate scheduler\n",
    "    warmup_ratio=0.05,  # Warmup ratio\n",
    "    weight_decay=0.01,  # Weight decay\n",
    "    save_total_limit=12,  # Keep only the last 2 checkpoints\n",
    "    fp16=torch.cuda.is_available(),  # Use FP16 if a GPU is available\n",
    "    seed=seed,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",  # Optimize for loss\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8650bfaa-80b9-43e9-a4a8-20293148e156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20604' max='20604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20604/20604 7:24:33, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.365858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.047800</td>\n",
       "      <td>0.355463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.366262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.369220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037400</td>\n",
       "      <td>0.384117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.387077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.392979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031200</td>\n",
       "      <td>0.396413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.412934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.027700</td>\n",
       "      <td>0.413032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.026500</td>\n",
       "      <td>0.426418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.025400</td>\n",
       "      <td>0.434826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-3434 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-6868 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [308/308 03:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'eval_loss': 0.355462908744812, 'eval_runtime': 206.8785, 'eval_samples_per_second': 47.574, 'eval_steps_per_second': 1.489, 'epoch': 12.0}\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained(\"./final_flan_t5_esnli\")\n",
    "tokenizer.save_pretrained(\"./final_flan_t5_esnli\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "print(\"Test results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "087891cc-a55d-49b3-8134-04f617160e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/generation/utils.py:1355: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9842, 9842)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foutputs = []\n",
    "flabels = []\n",
    "for i in range(9842):\n",
    "    finputs = eval_dataset[i]\n",
    "    # Get the model's output\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(finputs['input_ids'].cuda().unsqueeze(0), attention_mask=finputs['attention_mask'].cuda().unsqueeze(0))\n",
    "    # Decode the output (convert token IDs back to text)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Print the result\n",
    "    #print(f\"Generated output: {decoded_output}\")\n",
    "    foutputs.append(decoded_output)\n",
    "    flabels.append(dataset['validation'][i]['label'])\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "len(foutputs), len(flabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91c13784-87f0-4246-8cea-f10f5492dd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9842"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = []\n",
    "dct = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "for x in foutputs:\n",
    "    preds.append(dct[x.split(':')[0]])\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa233f98-78c8-4b9e-a198-1b67e87ebbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8238\n",
      "Precision (Macro): 0.8243\n",
      "Recall (Macro): 0.8235\n",
      "F1-score (Macro): 0.8237\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(flabels, preds)\n",
    "\n",
    "# Calculate precision, recall, and F1-score for each class (macro-average)\n",
    "precision = precision_score(flabels, preds, average='macro')\n",
    "recall = recall_score(flabels, preds, average='macro')\n",
    "f1 = f1_score(flabels, preds, average='macro')\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (Macro): {precision:.4f}\")\n",
    "print(f\"Recall (Macro): {recall:.4f}\")\n",
    "print(f\"F1-score (Macro): {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80366cf3-7388-4b51-9f89-d2feba9cfc71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
