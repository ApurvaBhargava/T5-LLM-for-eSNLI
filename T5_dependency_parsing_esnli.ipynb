{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf9586e-e749-4737-9a1b-270e9b26046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed) \n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12b1e16-6bc2-4b30-993a-e983d9ca23ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset esnli (/home/ec2-user/.cache/huggingface/datasets/esnli/plain_text/0.0.2/a160e6a02bbb8d828c738918dafec4e7d298782c334b5109af632fec6d779bbc)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6d1f0c89f3465ea7d9586e7881e995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(54937, 9842)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the e-SNLI dataset\n",
    "dataset = load_dataset(\"esnli\")\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['validation']\n",
    "#test_dataset = dataset['test']\n",
    "\n",
    "indices = list(range(0, len(train_dataset), 10))  # Select every 10th index\n",
    "train_dataset = train_dataset.select(indices)\n",
    "\n",
    "len(train_dataset), len(eval_dataset)#, len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40f6c5c7-dec3-4cf3-ba8d-2f2fa8c2745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency parsing\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def dependency_parse(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    # Generate the dependency tree structure\n",
    "    return \" \".join([f\"<{token.dep_}> {token.text}\" for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d760d271-19d3-433a-9422-cc1d54879a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<det> the <nsubj> car <aux> is <ROOT> moving <prep> on <det> the <pobj> road <punct> .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_parse(\"the car is moving on the road.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da75a00-1d66-42e9-a6f1-8f8ea98251d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dct = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c6867fc-8572-4f91-8f10-d1f7048b9196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b35b8e216044a889fde3aa91093c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54937 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8a262ca00d496789854f8b49478d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9842 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(example):\n",
    "    # Prepare input and output text\n",
    "    input_text = f\"Premise: example['premise'] Dependency: {dependency_parse(example['premise'])} Hypothesis: example['hypothesis'] Dependency: {dependency_parse(example['hypothesis'])} What is the relationship? Explain your answer.\"\n",
    "    output_text = f\"{label_dct[example['label']]}: {example['explanation_1']}. {example['explanation_2']}. {example['explanation_3']}.\"\n",
    "\n",
    "    # Tokenize input and output\n",
    "    input_encoding = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "    output_encoding = tokenizer(output_text, truncation=True, padding=\"max_length\", max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Create a dictionary to return\n",
    "    return {\n",
    "        \"input_ids\": input_encoding[\"input_ids\"][0],  # Remove batch dimension\n",
    "        \"attention_mask\": input_encoding[\"attention_mask\"][0],  # Remove batch dimension\n",
    "        \"labels\": output_encoding[\"input_ids\"][0] # Remove batch dimension\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\n",
    ")\n",
    "eval_dataset = eval_dataset.map(\n",
    "    preprocess,\n",
    "    remove_columns=['premise', 'hypothesis', 'label', 'explanation_1', 'explanation_2', 'explanation_3'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9756c8a8-9cca-4b0b-a67e-047621d88ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print(train_dataset[0]['input_ids'].shape)  # Should show (512,)\n",
    "print(train_dataset[0]['attention_mask'].shape)  # Should show (512,)\n",
    "print(train_dataset[0]['labels'].shape)  # Should show (512,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93b2aeaa-eaaf-4fa6-957b-92d697c48d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ddbe233-b06e-40bc-a674-622b6664bb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "# Define custom optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./flan_t5_esnli\",\n",
    "    evaluation_strategy=\"epoch\",  # Evaluate at the end of every epoch\n",
    "    save_strategy=\"epoch\",  # Save at the end of every epoch\n",
    "    per_device_train_batch_size=8,  # Train batch size\n",
    "    per_device_eval_batch_size=8,  # Evaluation batch size\n",
    "    num_train_epochs=12,  # Number of epochs\n",
    "    learning_rate=learning_rate,  # Learning rate\n",
    "    lr_scheduler_type=\"linear\",  # Linear learning rate scheduler\n",
    "    warmup_ratio=0.05,  # Warmup ratio\n",
    "    weight_decay=0.01,  # Weight decay\n",
    "    save_total_limit=12,  # Keep only the last 2 checkpoints\n",
    "    fp16=torch.cuda.is_available(),  # Use FP16 if a GPU is available\n",
    "    seed=seed,\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",  # Optimize for loss\n",
    "    greater_is_better=False,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c55b026-0e7b-4085-874d-4e4e6c0518a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20604' max='20604' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20604/20604 7:25:44, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.053600</td>\n",
       "      <td>0.861022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.048100</td>\n",
       "      <td>0.258028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.043500</td>\n",
       "      <td>0.296071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.040500</td>\n",
       "      <td>0.246219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.264902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.035600</td>\n",
       "      <td>0.393109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.336889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.031400</td>\n",
       "      <td>0.336917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.322672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.028000</td>\n",
       "      <td>0.334201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.330829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.336264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-1717 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-3434 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-5151 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-6868 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-8585 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-10302 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-12019 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-13736 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-15453 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-17170 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-18887 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Checkpoint destination directory ./flan_t5_esnli/checkpoint-20604 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [308/308 03:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test results: {'eval_loss': 0.2462185174226761, 'eval_runtime': 207.0557, 'eval_samples_per_second': 47.533, 'eval_steps_per_second': 1.488, 'epoch': 12.0}\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained(\"./final_flan_t5_esnli\")\n",
    "tokenizer.save_pretrained(\"./final_flan_t5_esnli\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "print(\"Test results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8edc340d-9cf6-4044-a014-f81127fb34ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "false = \"f\"\n",
    "true = \"t\"\n",
    "yoyo = {\n",
    "    \"FraudulentPaystubs::Templates::One\": false,\n",
    "    \"FraudulentPaystubs::Templates::Two\": false,\n",
    "    \"FraudulentPaystubs::Templates::Three\": false,\n",
    "    \"FraudulentPaystubs::Templates::Four\": true,\n",
    "    \"FraudulentPaystubs::Templates::Five\": false,\n",
    "    \"FraudulentPaystubs::Templates::Six\": false,\n",
    "    \"FraudulentPaystubs::Templates::Seven\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThePaystubsAdpAndRectangle\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThePaystubsSquare\": false,\n",
    "    \"FraudulentPaystubs::Templates::OnlinePaystubAdvanced\": false,\n",
    "    \"FraudulentPaystubs::Templates::PaystubOnlineNeat\": false,\n",
    "    \"FraudulentPaystubs::Templates::RealCheckstubsAdvThree\": false,\n",
    "    \"FraudulentPaystubs::Templates::PaystubDirect\": false,\n",
    "    \"FraudulentPaystubs::VisionDatas::SequenceNumbers\": false,\n",
    "    \"FraudulentPaystubs::Templates::Fifteen\": false,\n",
    "    \"FraudulentPaystubs::Templates::Sixteen\": false,\n",
    "    \"FraudulentPaystubs::Templates::Seventeen\": false,\n",
    "    \"FraudulentPaystubs::Templates::Eighteen\": false,\n",
    "    \"FraudulentPaystubs::Templates::Nineteen\": false,\n",
    "    \"FraudulentPaystubs::Templates::Twenty\": false,\n",
    "    \"FraudulentPaystubs::Templates::TwentyOne\": false,\n",
    "    \"FraudulentPaystubs::Templates::TwentyTwo\": false,\n",
    "    \"FraudulentPaystubs::Templates::TwentyThree\": false,\n",
    "    \"FraudulentPaystubs::Templates::TwentyFour\": false,\n",
    "    \"FraudulentPaystubs::Templates::TwentyFive\": false,\n",
    "    \"FraudulentPaystubs::Templates::TwentySix\": false,\n",
    "    \"FraudulentPaystubs::Templates::TwentySeven\": false,\n",
    "    \"FraudulentPaystubs::Templates::Thirty\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtyOne\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtyTwo\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtyThree\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtyFour\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtyFive\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtySix\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtySeven\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtyEight\": false,\n",
    "    \"FraudulentPaystubs::Templates::ThirtyNine\": false,\n",
    "    \"FraudulentPaystubs::Templates::Forty\": false,\n",
    "    \"FraudulentPaystubs::Templates::FortyOne\": false,\n",
    "    \"FraudulentPaystubs::Templates::FiftyOne\": false,\n",
    "    \"FraudulentPaystubs::Templates::FiftyFour\": false,\n",
    "    \"FraudulentPaystubs::Templates::FiftyFive\": false,\n",
    "    \"FraudulentPaystubs::Templates::FiftySeven\": false,\n",
    "    \"FraudulentPaystubs::Templates::FiftyNine\": false,\n",
    "    \"FraudulentPaystubs::Templates::SixtyOne\": false,\n",
    "    \"FraudulentPaystubs::Templates::SixtySeven\": false,\n",
    "    \"FraudulentPaystubs::Templates::SixtyEight\": false,\n",
    "    \"FraudulentPaystubs::Templates::SixtyNine\": false\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91d2b5f8-ef78-4ca0-93a8-1ba932445337",
   "metadata": {},
   "outputs": [],
   "source": [
    "yoyo = list(yoyo.keys())\n",
    "yoyo = {x: str(i+1) for i,x in enumerate(yoyo)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9988c79-78dd-40c4-ba2f-36fff3b4de61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"FraudulentPaystubs::Templates::One\": \"1\", \"FraudulentPaystubs::Templates::Two\": \"2\", \"FraudulentPaystubs::Templates::Three\": \"3\", \"FraudulentPaystubs::Templates::Four\": \"4\", \"FraudulentPaystubs::Templates::Five\": \"5\", \"FraudulentPaystubs::Templates::Six\": \"6\", \"FraudulentPaystubs::Templates::Seven\": \"7\", \"FraudulentPaystubs::Templates::ThePaystubsAdpAndRectangle\": \"8\", \"FraudulentPaystubs::Templates::ThePaystubsSquare\": \"9\", \"FraudulentPaystubs::Templates::OnlinePaystubAdvanced\": \"10\", \"FraudulentPaystubs::Templates::PaystubOnlineNeat\": \"11\", \"FraudulentPaystubs::Templates::RealCheckstubsAdvThree\": \"12\", \"FraudulentPaystubs::Templates::PaystubDirect\": \"13\", \"FraudulentPaystubs::VisionDatas::SequenceNumbers\": \"14\", \"FraudulentPaystubs::Templates::Fifteen\": \"15\", \"FraudulentPaystubs::Templates::Sixteen\": \"16\", \"FraudulentPaystubs::Templates::Seventeen\": \"17\", \"FraudulentPaystubs::Templates::Eighteen\": \"18\", \"FraudulentPaystubs::Templates::Nineteen\": \"19\", \"FraudulentPaystubs::Templates::Twenty\": \"20\", \"FraudulentPaystubs::Templates::TwentyOne\": \"21\", \"FraudulentPaystubs::Templates::TwentyTwo\": \"22\", \"FraudulentPaystubs::Templates::TwentyThree\": \"23\", \"FraudulentPaystubs::Templates::TwentyFour\": \"24\", \"FraudulentPaystubs::Templates::TwentyFive\": \"25\", \"FraudulentPaystubs::Templates::TwentySix\": \"26\", \"FraudulentPaystubs::Templates::TwentySeven\": \"27\", \"FraudulentPaystubs::Templates::Thirty\": \"28\", \"FraudulentPaystubs::Templates::ThirtyOne\": \"29\", \"FraudulentPaystubs::Templates::ThirtyTwo\": \"30\", \"FraudulentPaystubs::Templates::ThirtyThree\": \"31\", \"FraudulentPaystubs::Templates::ThirtyFour\": \"32\", \"FraudulentPaystubs::Templates::ThirtyFive\": \"33\", \"FraudulentPaystubs::Templates::ThirtySix\": \"34\", \"FraudulentPaystubs::Templates::ThirtySeven\": \"35\", \"FraudulentPaystubs::Templates::ThirtyEight\": \"36\", \"FraudulentPaystubs::Templates::ThirtyNine\": \"37\", \"FraudulentPaystubs::Templates::Forty\": \"38\", \"FraudulentPaystubs::Templates::FortyOne\": \"39\", \"FraudulentPaystubs::Templates::FiftyOne\": \"40\", \"FraudulentPaystubs::Templates::FiftyFour\": \"41\", \"FraudulentPaystubs::Templates::FiftyFive\": \"42\", \"FraudulentPaystubs::Templates::FiftySeven\": \"43\", \"FraudulentPaystubs::Templates::FiftyNine\": \"44\", \"FraudulentPaystubs::Templates::SixtyOne\": \"45\", \"FraudulentPaystubs::Templates::SixtySeven\": \"46\", \"FraudulentPaystubs::Templates::SixtyEight\": \"47\", \"FraudulentPaystubs::Templates::SixtyNine\": \"48\"}'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.dumps(yoyo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf27f7-2feb-47ee-96c1-adcd898d63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"FraudulentPaystubs::Templates::One\": \"1\", \"FraudulentPaystubs::Templates::Two\": \"2\", \"FraudulentPaystubs::Templates::Three\": \"3\", \"FraudulentPaystubs::Templates::Four\": \"4\", \"FraudulentPaystubs::Templates::Five\": \"5\", \"FraudulentPaystubs::Templates::Six\": \"6\", \"FraudulentPaystubs::Templates::Seven\": \"7\", \"FraudulentPaystubs::Templates::ThePaystubsAdpAndRectangle\": \"8\", \"FraudulentPaystubs::Templates::ThePaystubsSquare\": \"9\", \"FraudulentPaystubs::Templates::OnlinePaystubAdvanced\": \"10\", \"FraudulentPaystubs::Templates::PaystubOnlineNeat\": \"11\", \"FraudulentPaystubs::Templates::RealCheckstubsAdvThree\": \"12\", \"FraudulentPaystubs::Templates::PaystubDirect\": \"13\", \"FraudulentPaystubs::VisionDatas::SequenceNumbers\": \"14\", \"FraudulentPaystubs::Templates::Fifteen\": \"15\", \"FraudulentPaystubs::Templates::Sixteen\": \"16\", \"FraudulentPaystubs::Templates::Seventeen\": \"17\", \"FraudulentPaystubs::Templates::Eighteen\": \"18\", \"FraudulentPaystubs::Templates::Nineteen\": \"19\", \"FraudulentPaystubs::Templates::Twenty\": \"20\", \"FraudulentPaystubs::Templates::TwentyOne\": \"21\", \"FraudulentPaystubs::Templates::TwentyTwo\": \"22\", \"FraudulentPaystubs::Templates::TwentyThree\": \"23\", \"FraudulentPaystubs::Templates::TwentyFour\": \"24\", \"FraudulentPaystubs::Templates::TwentyFive\": \"25\", \"FraudulentPaystubs::Templates::TwentySix\": \"26\", \"FraudulentPaystubs::Templates::TwentySeven\": \"27\", \"FraudulentPaystubs::Templates::Thirty\": \"28\", \"FraudulentPaystubs::Templates::ThirtyOne\": \"29\", \"FraudulentPaystubs::Templates::ThirtyTwo\": \"30\", \"FraudulentPaystubs::Templates::ThirtyThree\": \"31\", \"FraudulentPaystubs::Templates::ThirtyFour\": \"32\", \"FraudulentPaystubs::Templates::ThirtyFive\": \"33\", \"FraudulentPaystubs::Templates::ThirtySix\": \"34\", \"FraudulentPaystubs::Templates::ThirtySeven\": \"35\", \"FraudulentPaystubs::Templates::ThirtyEight\": \"36\", \"FraudulentPaystubs::Templates::ThirtyNine\": \"37\", \"FraudulentPaystubs::Templates::Forty\": \"38\", \"FraudulentPaystubs::Templates::FortyOne\": \"39\", \"FraudulentPaystubs::Templates::FiftyOne\": \"40\", \"FraudulentPaystubs::Templates::FiftyFour\": \"41\", \"FraudulentPaystubs::Templates::FiftyFive\": \"42\", \"FraudulentPaystubs::Templates::FiftySeven\": \"43\", \"FraudulentPaystubs::Templates::FiftyNine\": \"44\", \"FraudulentPaystubs::Templates::SixtyOne\": \"45\", \"FraudulentPaystubs::Templates::SixtySeven\": \"46\", \"FraudulentPaystubs::Templates::SixtyEight\": \"47\", \"FraudulentPaystubs::Templates::SixtyNine\": \"48\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
